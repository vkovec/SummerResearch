\relax 
\citation{solwayoptimal}
\citation{pickett2002policyblocks}
\citation{still}
\citation{Puterman}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{\thepage }}
\citation{sutton1999between}
\citation{sutton1999between}
\citation{still}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Options}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Information Theory Aspect}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Predictive Information}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Kullback-Leibler Divergence}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Algorithm}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Experiments}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Grid Environments}{\thepage }}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pseudocode for policy-search algorithm\relax }}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Interesting area}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Results}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results of testing the algorithm on an obstacle-free environment with one large random option. In (a) and (c) the blue tile represents the starting state, the green tile represents the goal, and the black tile is the subgoal for the option. In (b) and (d), a darker red represents states that were visited more frequently.\relax }}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results of running the algorithm on the grid world shown in (a), with and without the interesting area. Here the interesting area represented a set of states where the reward would randomly change at each time step, according to a Gaussian distribution. In (b) and (c), a darker red represents states that were visited more frequently.\relax }}{\thepage }}
\bibstyle{abbrv}
\bibdata{references}
\bibcite{pickett2002policyblocks}{1}
\bibcite{Puterman}{2}
\bibcite{solwayoptimal}{3}
\bibcite{still}{4}
\bibcite{sutton1999between}{5}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusions}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}References}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results of running the algorithm on an environment with one large obstacle and two hand-crafted options. The results are averaged over 20 full runs of the algorithm.\relax }}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results of experiment where algorithm sifts through 16 handcrafted options. Options are completely removed when the policy does not take the option with a probability > 0.001 in any of the states in its initiation set.\relax }}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Results of creating smaller options from a large random option. States were removed from the option's initiation set if the state would take the option with a probability < 0.0001. In (b), a darker red represents states that were visited more frequently.\relax }}{\thepage }}
